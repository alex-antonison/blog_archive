<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Ethical use of algorithms with data | Alex Antonison’s Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Ethical use of algorithms with data" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The importance of ethics in the world of data science." />
<meta property="og:description" content="The importance of ethics in the world of data science." />
<link rel="canonical" href="https://alex-antonison.com/ethics/2019/03/09/ethical-use-of-algorithms-with-data.html" />
<meta property="og:url" content="https://alex-antonison.com/ethics/2019/03/09/ethical-use-of-algorithms-with-data.html" />
<meta property="og:site_name" content="Alex Antonison’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-03-09T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://alex-antonison.com/ethics/2019/03/09/ethical-use-of-algorithms-with-data.html","@type":"BlogPosting","headline":"Ethical use of algorithms with data","dateModified":"2019-03-09T00:00:00-06:00","datePublished":"2019-03-09T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://alex-antonison.com/ethics/2019/03/09/ethical-use-of-algorithms-with-data.html"},"description":"The importance of ethics in the world of data science.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://alex-antonison.com/feed.xml" title="Alex Antonison's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Alex Antonison&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Ethical use of algorithms with data</h1><p class="page-description">The importance of ethics in the world of data science.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-03-09T00:00:00-06:00" itemprop="datePublished">
        Mar 9, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#ethics">ethics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#ethical-use-of-algorithms-with-data">Ethical use of algorithms with data</a>
<ul>
<li class="toc-entry toc-h3"><a href="#preface">Preface</a></li>
<li class="toc-entry toc-h3"><a href="#gender-biased-word-embeddings">Gender biased word embeddings</a></li>
<li class="toc-entry toc-h3"><a href="#mortgage-loan-interest-rate-bias">Mortgage loan interest rate bias</a></li>
<li class="toc-entry toc-h3"><a href="#image-recognition-bias">Image recognition bias</a></li>
<li class="toc-entry toc-h3"><a href="#microsoft-tay-twitter-bot">Microsoft Tay twitter bot</a></li>
<li class="toc-entry toc-h3"><a href="#facebook-cambridge-analytica-scandal">Facebook Cambridge Analytica scandal</a></li>
<li class="toc-entry toc-h3"><a href="#ways-to-improve">Ways to improve</a></li>
<li class="toc-entry toc-h3"><a href="#additional-sources">Additional sources</a></li>
</ul>
</li>
</ul><h1 id="ethical-use-of-algorithms-with-data">
<a class="anchor" href="#ethical-use-of-algorithms-with-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ethical use of algorithms with data</h1>

<h3 id="preface">
<a class="anchor" href="#preface" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preface</h3>

<p>Before I talk about my views on ethics in the realm of Data Science, I first want to talk about how I got into Data Science.  I spent the first two years of my career doing some analysis accompanied by mostly Data Engineering before I knew it was Data Engineering.  At this two year mark in 2014, I was at a point where I wanted to figure out where I wanted to take my career next.  After a bit of searching, I landed on the field of Data Science since it seemed like a perfect fit with my love of statistics and working with data.  Like with anything, my first approach was to search and try and find as much information on the topic as possible.  I took a few Coursera courses, followed <em>top data scientists on twitter</em>, read blogs, and listened to podcasts.  More often than not, the topics were usually around applications of machine learning, interesting papers on machine learning models, or interviews with industry experts.  However, periodically there was a post or podcast that would catch my eye on a topic like “gender biased word embeddings.” I quickly realized that although Data Science has the propensity for good, it also has a potential to harm individuals or entire communities.  I began actively searching for these issues and came upon a couple of good articles that covered a wide variety of issues such as <a href="https://www.technologyreview.com/s/608248/biased-algorithms-are-everywhere-and-no-one-seems-to-care/">Biased Algorithms Are Everywhere, and No One Seems to Care</a></p>

<p>However, before I go any further, I would like to introduce two concepts that are at the center of this post.</p>

<ul>
  <li>
<strong>Machine Learning:</strong> “Machine learning algorithms build a mathematical model of sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to perform the task.” - <a href="https://en.wikipedia.org/wiki/Machine_learning">https://en.wikipedia.org/wiki/Machine_learning</a>
</li>
  <li>
<strong>Algorithmic Bias:</strong> “Algorithmic bias occurs when a computer system reflects the implicit values of the humans who are involved in coding, collecting, selecting, or using data to train the algorithm.” - <a href="https://en.wikipedia.org/wiki/Algorithmic_bias">https://en.wikipedia.org/wiki/Algorithmic_bias</a>
</li>
</ul>

<p>With that covered, I will move on to discuss a handful of cases where models are either biased, unethically created, or unethically used.</p>

<h3 id="gender-biased-word-embeddings">
<a class="anchor" href="#gender-biased-word-embeddings" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gender biased word embeddings</h3>

<p>Before I talk about how a <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> can be gender biased, first I would like to discuss what is a word embedding.  A word embedding is a useful natural language processing tool where a model can represent the relationships between words as mathematical values to allow for associations such as “man:king” with “woman:queen” and “paris:france” with “tokyo:japan”.  Cool, right? However, this bias has been extended to “man:programmer” with “woman:homemaker”. Not cool.  A common model used is Word2Vec developed by Google back in 2013 trained on Google News texts.  Unfortunately, because the data this model was trained on was gender biased, so are the results.  But there is hope! Researchers have done work to both quantify the bias and come up with methods to “debias” the embeddings in <a href="https://arxiv.org/abs/1607.06520">
Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a>.  For more on this, I recommend <a href="https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/">How Vector Space Mathematics Reveals the Hidden Sexism in Language</a>.</p>

<h3 id="mortgage-loan-interest-rate-bias">
<a class="anchor" href="#mortgage-loan-interest-rate-bias" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mortgage loan interest rate bias</h3>

<p>I find this to be a more traditional instance where financial institutions set out to use “big data” with “machine learning” to find ways to infer interest rates based on geography or characteristics of applicants.  This is referred to as “Algorithmic Strategic Pricing”. A result of this, <a href="http://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf">based on a study done by UC Berkly</a>, African Americans and Latino borrowers pay more on purchase and refinance loans than White and Asian ethnicity borrowers. “The lenders may not be specifically targeting minorities in their pricing schemes, but by profiling non-shopping applicants they end up targeting them” said <a href="http://faculty.haas.berkeley.edu/morse/">Adair Morse</a>.</p>

<p>For more information, you can check out <a href="http://newsroom.haas.berkeley.edu/minority-homebuyers-face-widespread-statistical-lending-discrimination-study-finds/">Minority homebuyers face widespread statistical lending discrimination, study finds</a> or the study itself <a href="http://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf">Consumer-Lending Discrimination in the Era of FinTech</a></p>

<h3 id="image-recognition-bias">
<a class="anchor" href="#image-recognition-bias" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image recognition bias</h3>

<p>Image recognition has become an everyday utility in society with many big tech companies using it in their product offerings like Google, Microsoft, and most notably, Facebook.  However, without any industry benchmarks to ensure that these facial recognition applications perform well on people of all races, genders, and age, there are instances where either the systems simply do not work or the systems are offensive.</p>

<p>A more notable instance of  a facial recognition system failing to work was discovered by Joy Buolamwini, who is an African American PhD student at MIT’s Center for Civic Media. At the time of her discovery, she was a Computer Science undergraduate at Georgia Tech.  In her undergrad, she was working on a research project to teach a computer to play “peek-a-boo” and found out that although the system had no issues recognizing her lighter skinned roommates, it had difficulty working with her.  Her solution to this was to wear a white Halloween mask which would then detect her as white. More on this can be found here <a href="https://www.pbs.org/wgbh/nova/article/ai-bias/">Ghosts in the Machine</a></p>

<p>This issue is not limited to Joy Buolamwini’s research, but could also be seen in Microsoft’s Kinect for the X-box.  Back in 2010, it was observed that Microsoft’s Kinect often times would not work on people with darker skin.  <a href="https://www.pcworld.com/article/209708/Is_Microsoft_Kinect_Racist.html">Is Microsoft’s Kinect Racist?</a>.  However, additionally worth noting is Microsoft advocating for there to be more regulation around image recognition in their blog post <a href="https://blogs.microsoft.com/on-the-issues/2018/07/13/facial-recognition-technology-the-need-for-public-regulation-and-corporate-responsibility/">Facial recognition technology: The need for public regulation and corporate responsibility</a>.</p>

<h3 id="microsoft-tay-twitter-bot">
<a class="anchor" href="#microsoft-tay-twitter-bot" aria-hidden="true"><span class="octicon octicon-link"></span></a>Microsoft Tay twitter bot</h3>

<p>I like using Microsoft’s Tay twitter bot as an example regarding ethics since it is an instance where the researchers themselves weren’t being unethical, but failed to consider how their model could be interacted with and manipulated.  To provide a brief summary, Microsoft’s Tay twitter bot was a research experiment conducted where they  built an artificial intelligence twitter bot that was supposed to learn how to mimic the speech of a 19 year old American girl by interacting with people on twitter.  However, what they failed to consider was a series of internet users  deciding to bombard the twitter bot with hateful speech.  The end result required Microsoft to turn the twitter bot off after 16 hours.</p>

<p>This is an instance where although the researchers themselves were not being unethical, they failed to take into consideration how their twitter bot could be manipulated.  As we build products, it is important not only to think about what the purpose of the model is but how it could be used to harm other people.</p>

<h3 id="facebook-cambridge-analytica-scandal">
<a class="anchor" href="#facebook-cambridge-analytica-scandal" aria-hidden="true"><span class="octicon octicon-link"></span></a>Facebook Cambridge Analytica scandal</h3>

<p>An ethics blog post would be incomplete without mentioning the  <a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal">Facebook–Cambridge Analytica data scandal</a>.  To briefly summarize, this is an instance where an organization used a survey app through Facebook to collect information from users for supposedly academic purposes.  However, through manipulating Facebook’s app design, they were also able to collect the information of not only the users that agreed to the survey, but all of the users’ friends information as well.   Furthermore, instead of using this information for academic purposes, they used it for both the Ted Cruz and Donald Trump political campaigns.</p>

<p>The two main takeaways here were that Cambridge Analytica both collected people’s information without their consent and then used the information for purposes  beyond the consent given.  Needless to say, collecting people’s information without their consent is clearly unethical.  However, even when  collecting people’s personal information  ethically , it is  important  that measures are taken to ensure their information is protected and not misused.</p>

<h3 id="ways-to-improve">
<a class="anchor" href="#ways-to-improve" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ways to improve</h3>

<p>A few closing thoughts on ways the Data Science industry can improve:</p>

<ul>
  <li>Build teams of people from diverse backgrounds to ensure underrepresented communities are not negatively impacted by biased models.</li>
  <li>Audit algorithms AND  the data sets used to train models.</li>
  <li>Encourage companies to provide more information to users and researchers to help them better understand potential pitfalls and biases that may exist in their tools.</li>
</ul>

<h3 id="additional-sources">
<a class="anchor" href="#additional-sources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Additional sources</h3>

<p>If you are still interested in looking more into this topic, I highly recommend checking out the following:</p>

<ul>
  <li>Articles
    <ul>
      <li><a href="https://www.technologyreview.com/s/610192/were-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our/">“We’re in a diversity crisis”: cofounder of Black in AI on what’s poisoning algorithms in our lives</a></li>
      <li><a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine bias risk assessments in criminal sentencing</a></li>
    </ul>
  </li>
  <li>Podcasts
    <ul>
      <li><a href="https://dataskeptic.com/blog/episodes/2018/data-ethics">Data Skeptic - Data Ethics</a></li>
      <li><a href="http://lineardigressions.com/episodes/2018/12/30/facial-recognition-society-and-you">Linear Digressions - Facial recognition, society, and the law</a></li>
      <li><a href="http://lineardigressions.com/episodes/2018/2/25/when-is-open-data-too-open">Linear Digressions - When is open data too open?</a></li>
    </ul>
  </li>
  <li>Books
    <ul>
      <li><a href="https://weaponsofmathdestructionbook.com/">Weapons of Math Destruction</a></li>
    </ul>
  </li>
</ul>

  </div><a class="u-url" href="/ethics/2019/03/09/ethical-use-of-algorithms-with-data.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog about working with data and machine learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/alex-antonison" title="alex-antonison"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/adantonison" title="adantonison"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
